name: Move DAGs to GCS

on:
  push:
    branches:
      - dev
  workflow_dispatch: # Enable manual trigger

jobs:
  move-dags:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Move DAG files to Cloud Composer
        run: |
          # Create a temporary directory to flatten the DAG structure
          mkdir -p /tmp/dags

          # Find all Python files in the src/dags directory and its subdirectories
          find src/dags -type f -name "*.py" | while read -r dag_file; do
            # Extract the filename without the path
            filename=$(basename "$dag_file")
            echo "Processing $dag_file"

            # Copy to temporary directory
            cp "$dag_file" "/tmp/dags/$filename"
          done

          # List all files in the temporary directory
          echo "Files to be uploaded:"
          ls -la /tmp/dags

          # Upload all DAGs at once to preserve file structure
          gcloud composer environments storage dags upload \
            --environment test-composer-for-dataproc \
            --location us-central1 \
            --source /tmp/dags

      - name: Move Init script to GCS
        run: |
          gcloud storage cp src/dags/init/dataproc_initialization_action.sh \
            gs://stage-yral-ds-dataproc-bucket/scripts/dataproc_initialization_action.sh
