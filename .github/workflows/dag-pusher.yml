name: Move DAGs to GCS

on:
  push:
    branches: [dev, main, master, stage-to-prod]
  workflow_dispatch: # Enable manual trigger

jobs:
  move-dags:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v1

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.RECSYS_GCP_CREDENTIALS }}

      - name: Move DAG files to Cloud Composer
        run: |
          # Create a temporary directory to flatten the DAG structure
          mkdir -p /tmp/dags

          # List of DAG directories to process
          # Add new DAG folders here when needed (comma-separated, no trailing comma)
          dag_directories=(
            "src/dags/user_clustering"
            # "src/dags/candidate_generation"
            # "src/dags/cache_refresh"
            # "src/dags/yet_another_dag_folder"
          )

          # Process all DAG directories
          for dag_dir in "${dag_directories[@]}"; do
            echo "Processing directory: $dag_dir"

            # Find all Python files in the specified directory and its subdirectories
            find "$dag_dir" -type f -name "*.py" | while read -r dag_file; do
              # Extract the filename without the path
              filename=$(basename "$dag_file")
              echo "Processing $dag_file"

              # Copy to temporary directory
              cp "$dag_file" "/tmp/dags/$filename"
            done
          done

          # List all files in the temporary directory
          echo "Files to be uploaded:"
          ls -la /tmp/dags

          # Copy each file individually to the dags folder
          for file in /tmp/dags/*.py; do
            filename=$(basename "$file")
            echo "Copying $filename to Cloud Composer"
            gcloud composer environments storage dags import \
              --environment data-pipeline-orchestrator \
              --location us-central1 \
              --source "$file"
          done

      - name: Move Init script to GCS
        run: |
          # Update this path if the initialization script location changes
          init_script_path="src/dags/user_clustering/create_dataproc_cluster/init/dataproc_initialization_action.sh"

          echo "Copying initialization script from $init_script_path to GCS"
          gcloud storage cp "$init_script_path" \
            gs://yral-dataproc-notebooks/yral-dataproc-notebooks/dataproc-initialization/dataproc_initialization_action.sh
