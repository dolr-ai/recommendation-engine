"""
Input:
- merged_user_embeddings.parquet (output from merge_part_embeddings.py)

Output:
- Uploads the user embeddings to a BigQuery table

This script loads the merged user embeddings generated by the Dataproc cluster
and uploads them to a BigQuery table for further analysis and serving.
"""

import os
import json
from datetime import datetime
import pandas as pd
from pyspark.sql import SparkSession

from utils.gcp_utils import GCPUtils


def load_embeddings(local_path):
    """
    Load the merged user embeddings from the local parquet file.

    Args:
        local_path: Path to the merged_user_embeddings.parquet file

    Returns:
        pandas DataFrame with the user embeddings
    """
    print(f"Loading user embeddings from {local_path}")

    # Initialize Spark Session for reading Parquet
    spark = SparkSession.builder.appName("Load User Embeddings").getOrCreate()

    # Load the parquet file
    df_spark = spark.read.parquet(local_path)

    # Convert to pandas DataFrame
    df = df_spark.toPandas()

    print(f"Loaded {len(df)} user embeddings")
    return df


def upload_to_bigquery(df, gcp_utils, dataset_id, table_id):
    """
    Upload the user embeddings to BigQuery.

    Args:
        df: DataFrame with user embeddings
        gcp_utils: GCPUtils instance
        dataset_id: BigQuery dataset ID
        table_id: BigQuery table ID
    """
    print(
        f"Uploading {len(df)} user embeddings to BigQuery table {dataset_id}.{table_id}"
    )

    # Define schema for the user embeddings table
    schema = [
        {
            "name": "user_id",
            "type": "STRING",
            "mode": "REQUIRED",
            "description": "Unique user identifier",
        },
        {
            "name": "avg_interaction_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Average interaction embedding vector from video interactions",
        },
        {
            "name": "temporal_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Temporal interaction embedding vector",
        },
        {
            "name": "cluster_distribution_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Distribution of user video cluster interactions",
        },
        {
            "name": "user_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Normalized concatenated user embedding vector",
        },
        {
            "name": "upload_timestamp",
            "type": "TIMESTAMP",
            "mode": "REQUIRED",
            "description": "Timestamp when the embedding was uploaded",
        },
    ]

    # Add upload timestamp to the DataFrame
    df["upload_timestamp"] = datetime.utcnow()

    # Upload the DataFrame to BigQuery
    gcp_utils.bigquery.upload_dataframe_to_table(
        df=df,
        dataset_id=dataset_id,
        table_id=table_id,
        if_exists="replace",  # Replace the existing table
        schema_updates=schema,
    )

    print("Successfully uploaded user embeddings to BigQuery")


def main():
    """Main execution function"""
    # Define paths and table info
    data_root = os.environ.get(
        "DATA_ROOT", "/home/dataproc/recommendation-engine/data_root"
    )
    local_embeddings_path = f"{data_root}/emb_analysis/merged_user_embeddings.parquet"

    # Load GCP credentials from environment or file
    credentials_path = os.environ.get("GCP_CREDENTIALS_PATH", "")
    credentials_json = ""

    if credentials_path and os.path.exists(credentials_path):
        with open(credentials_path, "r") as f:
            credentials_json = f.read()
    else:
        credentials_json = os.environ.get("GCP_CREDENTIALS_JSON", "")

    if not credentials_json:
        raise ValueError("GCP credentials not found in environment variables or file")

    # Define BigQuery table info
    dataset_id = "jay-dhanwant-experiments"
    table_id = "stage_test_tables.test_user_cluster_embeddings"

    # Initialize GCP utils
    gcp_utils = GCPUtils(gcp_credentials=credentials_json)

    # Load the embeddings
    df_embeddings = load_embeddings(local_embeddings_path)

    # Upload to BigQuery
    upload_to_bigquery(df_embeddings, gcp_utils, dataset_id, table_id)

    print("Process completed successfully")


if __name__ == "__main__":
    main()
