"""
Input:
- user_clusters.parquet (output from get_user_clusters.py)

Output:
- Uploads the user embeddings with cluster IDs to BigQuery table
- Copies the data_root directory to GCS bucket for debugging

This script loads the user clusters data generated by the Dataproc cluster
and uploads it to a BigQuery table for further analysis and serving.
"""

import os
import subprocess
from datetime import datetime
import json
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import (
    ArrayType,
    StructType,
    StringType,
    TimestampType,
    FloatType,
    BooleanType,
    IntegerType,
)

from utils.gcp_utils import GCPUtils
from utils.common_utils import path_exists

DATA_ROOT = "/home/dataproc/recommendation-engine/data_root"


def load_user_clusters(local_path):
    """
    Load the user clusters data from the local parquet file.

    Args:
        local_path: Path to the user_clusters.parquet file

    Returns:
        pandas DataFrame with the user clusters data
    """
    print(f"Loading user clusters data from {local_path}")

    # Initialize Spark Session for reading Parquet
    spark = SparkSession.builder.appName("Load User Clusters").getOrCreate()

    # Load the parquet file
    df_spark = spark.read.parquet(local_path)

    # Define schema for engagement_metadata
    engagement_metadata_schema = StructType(
        [
            ("video_id", StringType()),
            ("last_watched_timestamp", TimestampType()),
            ("mean_percentage_watched", FloatType()),
            ("liked", BooleanType()),
            ("last_liked_timestamp", TimestampType()),
            ("shared", BooleanType()),
            ("last_shared_timestamp", TimestampType()),
            ("cluster_label", IntegerType()),
        ]
    )

    # Convert engagement_metadata_list to JSON string to avoid timestamp conversion issues
    df_spark = df_spark.withColumn(
        "engagement_metadata_list", F.to_json(F.col("engagement_metadata_list"))
    )

    # Select all required columns including all embeddings
    df_spark = df_spark.select(
        "user_id",
        F.col("cluster").alias("cluster_id"),
        "user_embedding",
        "avg_interaction_embedding",
        "cluster_distribution_embedding",
        "temporal_embedding",
        "engagement_metadata_list",
    )

    # Convert to pandas DataFrame
    df = df_spark.toPandas()

    # Parse the JSON strings back to Python lists
    df["engagement_metadata_list"] = df["engagement_metadata_list"].apply(
        lambda x: json.loads(x) if x else []
    )

    print(f"Loaded {len(df)} user clusters")
    return df


def upload_to_bigquery(df, gcp_utils, dataset_id, table_id):
    """
    Upload the user clusters to BigQuery.

    Args:
        df: DataFrame with user clusters
        gcp_utils: GCPUtils instance
        dataset_id: BigQuery dataset ID
        table_id: BigQuery table ID
    """
    print(
        f"Uploading {len(df)} user clusters to BigQuery table {dataset_id}.{table_id}"
    )

    # Define schema for the user clusters table
    schema = [
        {
            "name": "user_id",
            "type": "STRING",
            "mode": "REQUIRED",
            "description": "Unique user identifier",
        },
        {
            "name": "cluster_id",
            "type": "INTEGER",
            "mode": "NULLABLE",
            "description": "Cluster identifier for partitioning and performance optimization",
        },
        {
            "name": "user_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Normalized concatenated user embedding vector",
        },
        {
            "name": "avg_interaction_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Average interaction embedding vector",
        },
        {
            "name": "temporal_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Temporal interaction embedding vector",
        },
        {
            "name": "cluster_distribution_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Cluster distribution embedding vector",
        },
        {
            "name": "engagement_metadata_list",
            "type": "RECORD",
            "mode": "REPEATED",
            "description": "List of video engagement metadata for user",
            "fields": [
                {"name": "video_id", "type": "STRING", "mode": "NULLABLE"},
                {
                    "name": "last_watched_timestamp",
                    "type": "TIMESTAMP",
                    "mode": "NULLABLE",
                },
                {
                    "name": "mean_percentage_watched",
                    "type": "FLOAT64",
                    "mode": "NULLABLE",
                },
                {"name": "liked", "type": "BOOLEAN", "mode": "NULLABLE"},
                {
                    "name": "last_liked_timestamp",
                    "type": "TIMESTAMP",
                    "mode": "NULLABLE",
                },
                {"name": "shared", "type": "BOOLEAN", "mode": "NULLABLE"},
                {
                    "name": "last_shared_timestamp",
                    "type": "TIMESTAMP",
                    "mode": "NULLABLE",
                },
                {"name": "cluster_label", "type": "INTEGER", "mode": "NULLABLE"},
            ],
        },
        {
            "name": "updated_at",
            "type": "TIMESTAMP",
            "mode": "REQUIRED",
            "description": "Timestamp when the embedding was uploaded",
        },
    ]

    # Add updated_at timestamp to the DataFrame
    df["updated_at"] = datetime.utcnow()

    # Ensure engagement_metadata_list is properly formatted
    # This step handles any timestamp conversions that might be needed
    for i, row in enumerate(df["engagement_metadata_list"]):
        for item in row:
            # Convert timestamp strings to datetime objects if needed
            for ts_field in [
                "last_watched_timestamp",
                "last_liked_timestamp",
                "last_shared_timestamp",
            ]:
                if (
                    ts_field in item
                    and item[ts_field]
                    and isinstance(item[ts_field], str)
                ):
                    try:
                        item[ts_field] = datetime.fromisoformat(
                            item[ts_field].replace("Z", "+00:00")
                        )
                    except (ValueError, AttributeError):
                        # If conversion fails, set to None
                        item[ts_field] = None

    # Upload the DataFrame to BigQuery
    gcp_utils.bigquery.upload_dataframe_to_table(
        df=df,
        dataset_id=dataset_id,
        table_id=table_id,
        if_exists="replace",  # Replace the existing table
        schema_updates=schema,
    )

    print("Successfully uploaded user clusters to BigQuery")


def main():
    """Main execution function"""
    # First check if file exists in HDFS

    # Check both potential locations
    hdfs_path = "/tmp/transformed/user_clusters/user_clusters.parquet"
    local_path = f"{DATA_ROOT}/transformed/user_clusters/user_clusters.parquet"

    # Check if file exists in HDFS
    hdfs_check = subprocess.run(
        ["hdfs", "dfs", "-test", "-e", hdfs_path], capture_output=True
    )
    hdfs_exists = hdfs_check.returncode == 0

    # Check if file exists locally
    local_exists = path_exists(local_path)

    print(f"HDFS path exists: {hdfs_exists}, Local path exists: {local_exists}")

    # Use the path that exists
    if hdfs_exists:
        user_clusters_path = hdfs_path
        print(f"Using HDFS path: {hdfs_path}")
    elif local_exists:
        user_clusters_path = local_path
        print(f"Using local path: {local_path}")
    else:
        # Create the directory if it doesn't exist
        subprocess.call(
            ["hdfs", "dfs", "-mkdir", "-p", "/tmp/transformed/user_clusters"]
        )
        # List directories to debug
        print("Available directories in HDFS:")
        subprocess.call(["hdfs", "dfs", "-ls", "/tmp/transformed"])
        print("Available local directories:")
        subprocess.call(
            ["ls", "-la", "/home/dataproc/recommendation-engine/data_root/transformed/"]
        )

        raise FileNotFoundError(
            f"User clusters file not found at {hdfs_path} or {local_path}"
        )

    # todo: remove this hardcoded path
    # Load GCP credentials from environment or file
    credentials_path = "/home/dataproc/recommendation-engine/credentials_stage.json"
    credentials_json = ""

    if credentials_path and os.path.exists(credentials_path):
        with open(credentials_path, "r") as f:
            credentials_json = f.read()
    else:
        credentials_json = os.environ.get("GCP_CREDENTIALS_JSON", "")

    if not credentials_json:
        raise ValueError("GCP credentials not found in environment variables or file")

    # Define BigQuery table info
    dataset_id = "stage_test_tables"
    table_id = "test_user_cluster_embeddings"

    # Initialize GCP utils
    gcp_utils = GCPUtils(gcp_credentials=credentials_json)

    # Load the user clusters data
    df_clusters = load_user_clusters(user_clusters_path)

    # Upload to BigQuery
    upload_to_bigquery(df_clusters, gcp_utils, dataset_id, table_id)

    # Upload data_root to GCS bucket - use the same gcp_utils object
    source_path = DATA_ROOT
    destination_path = "gs://stage-yral-ds-dataproc-bucket/data_dev_debug/"
    success = gcp_utils.storage.upload_directory(
        source_path,
        destination_path,
        max_workers=16,  # Use more workers for faster upload
    )

    if success:
        print(f"Successfully uploaded {source_path} to {destination_path}")
    else:
        print(f"Failed to upload {source_path} to {destination_path}")

    print("Process completed successfully")


if __name__ == "__main__":
    main()
