"""
Input:
- user_clusters.parquet (output from get_user_clusters.py)

Output:
- Uploads the user embeddings to BigQuery table test_user_embeddings
- Uploads the flattened engagement metadata to BigQuery table test_user_clusters
- Copies the data_root directory to GCS bucket for debugging

This script loads the user clusters data generated by the Dataproc cluster
and uploads it to BigQuery tables for further analysis and serving.
"""

import os
import subprocess
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *

from utils.gcp_utils import GCPUtils
from utils.common_utils import path_exists

DATA_ROOT = "/home/dataproc/recommendation-engine/data_root"


def load_user_clusters(local_path):
    """
    Load the user clusters data from the local parquet file.

    Args:
        local_path: Path to the user_clusters.parquet file

    Returns:
        Two PySpark DataFrames:
        - user_embeddings: containing user IDs, cluster IDs, and embeddings
        - user_clusters: containing flattened engagement metadata
    """
    print(f"Loading user clusters data from {local_path}")

    # Initialize Spark Session for reading Parquet
    spark = SparkSession.builder.appName("Load User Clusters").getOrCreate()

    # Load the parquet file
    df_spark = spark.read.parquet(local_path)

    # Current timestamp for both tables
    current_timestamp = F.current_timestamp()

    # Create user embeddings DataFrame (Table 1)
    df_user_embeddings = df_spark.select(
        F.col("cluster").alias("cluster_id"),
        "user_id",
        "user_embedding",
        "avg_interaction_embedding",
        "cluster_distribution_embedding",
        "temporal_embedding",
    ).withColumn("updated_at", current_timestamp)

    print(f"Created user embeddings dataframe")

    # Create user clusters DataFrame with flattened engagement metadata (Table 2)
    # First, explode the engagement_metadata_list to create one row per video
    df_user_clusters = (
        df_spark.select(
            F.col("cluster").alias("cluster_id"),
            "user_id",
            F.explode("engagement_metadata_list").alias("engagement"),
        )
        .select(
            "cluster_id",
            "user_id",
            F.col("engagement.video_id").alias("video_id"),
            F.col("engagement.last_watched_timestamp").alias("last_watched_timestamp"),
            F.col("engagement.mean_percentage_watched").alias(
                "mean_percentage_watched"
            ),
            F.col("engagement.liked").alias("liked"),
            F.col("engagement.last_liked_timestamp").alias("last_liked_timestamp"),
            F.col("engagement.shared").alias("shared"),
            F.col("engagement.last_shared_timestamp").alias("last_shared_timestamp"),
            F.col("engagement.cluster_label").alias("cluster_label"),
        )
        .withColumn("updated_at", current_timestamp)
    )

    print(f"Created user clusters dataframe with flattened engagement metadata")

    return df_user_embeddings, df_user_clusters


def upload_to_bigquery(df_spark, gcp_utils, dataset_id, table_id, credentials_path):
    """
    Upload a DataFrame to BigQuery directly from PySpark using the
    BigQuery Storage Write API.

    Args:
        df_spark: PySpark DataFrame to upload
        gcp_utils: GCPUtils instance
        dataset_id: BigQuery dataset ID
        table_id: BigQuery table ID
        credentials_path: Path to GCP credentials file
    """
    print(f"Uploading PySpark DataFrame to BigQuery table {dataset_id}.{table_id}")

    # Full table reference
    table_ref = f"{gcp_utils.core.project_id}.{dataset_id}.{table_id}"

    # Get the project ID
    project_id = gcp_utils.core.project_id

    # Print information for debugging
    print(f"Project ID: {project_id}")
    print(f"Target table: {table_ref}")
    print(f"DataFrame schema: {df_spark.schema}")

    # Set Spark configuration for BigQuery
    spark = df_spark.sparkSession
    spark.conf.set("parentProject", project_id)
    spark.conf.set("credentialsFile", credentials_path)

    # Write to BigQuery using the direct write API
    df_spark.write.format("bigquery").option("table", table_ref).option(
        "temporaryGcsBucket", "stage-yral-ds-dataproc-bucket"
    ).option("writeMethod", "direct").mode("overwrite").save()

    print(f"Successfully uploaded data to BigQuery table {table_id}")


def main():
    """Main execution function"""
    # First check if file exists in HDFS

    # Check both potential locations
    hdfs_path = "/tmp/transformed/user_clusters/user_clusters.parquet"
    local_path = f"{DATA_ROOT}/transformed/user_clusters/user_clusters.parquet"

    # Check if file exists in HDFS
    hdfs_check = subprocess.run(
        ["hdfs", "dfs", "-test", "-e", hdfs_path], capture_output=True
    )
    hdfs_exists = hdfs_check.returncode == 0

    # Check if file exists locally
    local_exists = path_exists(local_path)

    print(f"HDFS path exists: {hdfs_exists}, Local path exists: {local_exists}")

    # Use the path that exists
    if hdfs_exists:
        user_clusters_path = hdfs_path
        print(f"Using HDFS path: {hdfs_path}")
    elif local_exists:
        user_clusters_path = local_path
        print(f"Using local path: {local_path}")
    else:
        # Create the directory if it doesn't exist
        subprocess.call(
            ["hdfs", "dfs", "-mkdir", "-p", "/tmp/transformed/user_clusters"]
        )
        # List directories to debug
        print("Available directories in HDFS:")
        subprocess.call(["hdfs", "dfs", "-ls", "/tmp/transformed"])
        print("Available local directories:")
        subprocess.call(
            ["ls", "-la", "/home/dataproc/recommendation-engine/data_root/transformed/"]
        )

        raise FileNotFoundError(
            f"User clusters file not found at {hdfs_path} or {local_path}"
        )

    # todo: remove this hardcoded path
    # Load GCP credentials from environment or file
    credentials_path = "/home/dataproc/recommendation-engine/credentials_stage.json"
    credentials_json = ""

    if credentials_path and os.path.exists(credentials_path):
        with open(credentials_path, "r") as f:
            credentials_json = f.read()
    else:
        credentials_json = os.environ.get("GCP_CREDENTIALS_JSON", "")

    if not credentials_json:
        raise ValueError("GCP credentials not found in environment variables or file")

    # Define BigQuery dataset ID
    dataset_id = "stage_test_tables"

    # Table IDs for the two tables
    embeddings_table_id = "test_user_embeddings"
    clusters_table_id = "test_user_clusters"

    # Initialize GCP utils
    gcp_utils = GCPUtils(gcp_credentials=credentials_json)

    # Load the user clusters data into two separate DataFrames
    df_embeddings, df_clusters = load_user_clusters(user_clusters_path)

    # Print dimensions of each embedding type
    print("\nEmbedding dimensions in push_user_embedding_clusters.py:")

    # Convert to pandas to easily access the first row
    sample_row = df_embeddings.limit(1).toPandas()

    if not sample_row.empty:
        print(f"user_embedding dimensions: {len(sample_row['user_embedding'].iloc[0])}")
        print(
            f"avg_interaction_embedding dimensions: {len(sample_row['avg_interaction_embedding'].iloc[0])}"
        )
        print(
            f"temporal_embedding dimensions: {len(sample_row['temporal_embedding'].iloc[0])}"
        )
        print(
            f"cluster_distribution_embedding dimensions: {len(sample_row['cluster_distribution_embedding'].iloc[0])}"
        )
    else:
        print("No data available to print dimensions")

    # Upload user embeddings to BigQuery
    upload_to_bigquery(
        df_embeddings, gcp_utils, dataset_id, embeddings_table_id, credentials_path
    )
    print(f"Number of rows in user embeddings dataframe: {df_embeddings.count()}")
    print("Sample of embeddings data written:")
    df_embeddings.limit(5).show(truncate=False)

    # Upload user clusters to BigQuery
    upload_to_bigquery(
        df_clusters, gcp_utils, dataset_id, clusters_table_id, credentials_path
    )
    print(f"Number of rows in user clusters dataframe: {df_clusters.count()}")
    print("Sample of clusters data written:")
    df_clusters.limit(5).show(truncate=False)

    print("Process completed successfully")


if __name__ == "__main__":
    main()
