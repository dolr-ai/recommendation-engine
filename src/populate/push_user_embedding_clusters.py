"""
Input:
- user_clusters.parquet (output from get_user_clusters.py)

Output:
- Uploads the user embeddings with cluster IDs to BigQuery table
- Copies the data_root directory to GCS bucket for debugging

This script loads the user clusters data generated by the Dataproc cluster
and uploads it to a BigQuery table for further analysis and serving.
"""

import os
import subprocess
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

from utils.gcp_utils import GCPUtils
from utils.common_utils import path_exists

DATA_ROOT = "/home/dataproc/recommendation-engine/data_root"


def load_user_clusters(local_path):
    """
    Load the user clusters data from the local parquet file.

    Args:
        local_path: Path to the user_clusters.parquet file

    Returns:
        PySpark DataFrame with the user clusters data
    """
    print(f"Loading user clusters data from {local_path}")

    # Initialize Spark Session for reading Parquet
    spark = SparkSession.builder.appName("Load User Clusters").getOrCreate()

    # Load the parquet file
    df_spark = spark.read.parquet(local_path)

    # todo: remove embedding columns if not needed
    # Select all required columns including all embeddings
    df_spark = df_spark.select(
        "user_id",
        F.col("cluster").alias("cluster_id"),
        "user_embedding",
        "avg_interaction_embedding",
        "cluster_distribution_embedding",
        "temporal_embedding",
        "engagement_metadata_list",
    )

    # Add updated_at timestamp
    df_spark = df_spark.withColumn("updated_at", F.current_timestamp())

    print(f"Loaded user clusters dataframe")
    return df_spark


def upload_to_bigquery(df_spark, gcp_utils, dataset_id, table_id, credentials_path):
    """
    Upload the user clusters to BigQuery directly from PySpark using the
    BigQuery Storage Write API.

    Args:
        df_spark: PySpark DataFrame with user clusters
        gcp_utils: GCPUtils instance
        dataset_id: BigQuery dataset ID
        table_id: BigQuery table ID
    """
    print(f"Uploading PySpark DataFrame to BigQuery table {dataset_id}.{table_id}")

    # Full table reference
    table_ref = f"{gcp_utils.core.project_id}.{dataset_id}.{table_id}"

    # Get the project ID
    project_id = gcp_utils.core.project_id

    # Print information for debugging
    print(f"Project ID: {project_id}")
    print(f"Target table: {table_ref}")
    print(f"DataFrame schema: {df_spark.schema}")

    # Set Spark configuration for BigQuery
    spark = df_spark.sparkSession
    spark.conf.set("parentProject", project_id)
    spark.conf.set("credentialsFile", credentials_path)

    # Write to BigQuery using the direct write API
    df_spark.write.format("bigquery").option("table", table_ref).option(
        "temporaryGcsBucket", "stage-yral-ds-dataproc-bucket"
    ).option("writeMethod", "direct").mode("overwrite").save()

    print("Successfully uploaded user clusters to BigQuery")


def main():
    """Main execution function"""
    # First check if file exists in HDFS

    # Check both potential locations
    hdfs_path = "/tmp/transformed/user_clusters/user_clusters.parquet"
    local_path = f"{DATA_ROOT}/transformed/user_clusters/user_clusters.parquet"

    # Check if file exists in HDFS
    hdfs_check = subprocess.run(
        ["hdfs", "dfs", "-test", "-e", hdfs_path], capture_output=True
    )
    hdfs_exists = hdfs_check.returncode == 0

    # Check if file exists locally
    local_exists = path_exists(local_path)

    print(f"HDFS path exists: {hdfs_exists}, Local path exists: {local_exists}")

    # Use the path that exists
    if hdfs_exists:
        user_clusters_path = hdfs_path
        print(f"Using HDFS path: {hdfs_path}")
    elif local_exists:
        user_clusters_path = local_path
        print(f"Using local path: {local_path}")
    else:
        # Create the directory if it doesn't exist
        subprocess.call(
            ["hdfs", "dfs", "-mkdir", "-p", "/tmp/transformed/user_clusters"]
        )
        # List directories to debug
        print("Available directories in HDFS:")
        subprocess.call(["hdfs", "dfs", "-ls", "/tmp/transformed"])
        print("Available local directories:")
        subprocess.call(
            ["ls", "-la", "/home/dataproc/recommendation-engine/data_root/transformed/"]
        )

        raise FileNotFoundError(
            f"User clusters file not found at {hdfs_path} or {local_path}"
        )

    # todo: remove this hardcoded path
    # Load GCP credentials from environment or file
    credentials_path = "/home/dataproc/recommendation-engine/credentials_stage.json"
    credentials_json = ""

    if credentials_path and os.path.exists(credentials_path):
        with open(credentials_path, "r") as f:
            credentials_json = f.read()
    else:
        credentials_json = os.environ.get("GCP_CREDENTIALS_JSON", "")

    if not credentials_json:
        raise ValueError("GCP credentials not found in environment variables or file")

    # Define BigQuery table info
    dataset_id = "stage_test_tables"
    table_id = "test_user_cluster_embeddings"

    # Initialize GCP utils
    gcp_utils = GCPUtils(gcp_credentials=credentials_json)

    # Load the user clusters data
    df_clusters = load_user_clusters(user_clusters_path)

    # Upload to BigQuery
    upload_to_bigquery(df_clusters, gcp_utils, dataset_id, table_id, credentials_path)

    # Upload data_root to GCS bucket - use the same gcp_utils object
    source_path = DATA_ROOT
    destination_path = "gs://stage-yral-ds-dataproc-bucket/data_dev_debug/"
    success = gcp_utils.storage.upload_directory(
        source_path,
        destination_path,
        max_workers=16,  # Use more workers for faster upload
    )

    if success:
        print(f"Successfully uploaded {source_path} to {destination_path}")
    else:
        print(f"Failed to upload {source_path} to {destination_path}")

    print("Process completed successfully")


if __name__ == "__main__":
    main()
