"""
Input:
- user_clusters.parquet (output from get_user_clusters.py)

Output:
- Uploads the user embeddings with cluster IDs to BigQuery table

This script loads the user clusters data generated by the Dataproc cluster
and uploads it to a BigQuery table for further analysis and serving.
"""

import os
import json
from datetime import datetime
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

from utils.gcp_utils import GCPUtils


def load_user_clusters(local_path):
    """
    Load the user clusters data from the local parquet file.

    Args:
        local_path: Path to the user_clusters.parquet file

    Returns:
        pandas DataFrame with the user clusters data
    """
    print(f"Loading user clusters data from {local_path}")

    # Initialize Spark Session for reading Parquet
    spark = SparkSession.builder.appName("Load User Clusters").getOrCreate()

    # Load the parquet file
    df_spark = spark.read.parquet(local_path)

    # Select only the required columns
    df_spark = df_spark.select(
        "user_id", F.col("cluster").alias("cluster_id"), "user_embedding"
    )

    # Convert to pandas DataFrame
    df = df_spark.toPandas()

    print(f"Loaded {len(df)} user clusters")
    return df


def upload_to_bigquery(df, gcp_utils, dataset_id, table_id):
    """
    Upload the user clusters to BigQuery.

    Args:
        df: DataFrame with user clusters
        gcp_utils: GCPUtils instance
        dataset_id: BigQuery dataset ID
        table_id: BigQuery table ID
    """
    print(
        f"Uploading {len(df)} user clusters to BigQuery table {dataset_id}.{table_id}"
    )

    # Define schema for the user clusters table
    schema = [
        {
            "name": "user_id",
            "type": "STRING",
            "mode": "REQUIRED",
            "description": "Unique user identifier",
        },
        {
            "name": "cluster_id",
            "type": "INTEGER",
            "mode": "NULLABLE",
            "description": "Cluster identifier for partitioning and performance optimization",
        },
        {
            "name": "user_embedding",
            "type": "FLOAT64",
            "mode": "REPEATED",
            "description": "Normalized concatenated user embedding vector",
        },
        {
            "name": "updated_at",
            "type": "TIMESTAMP",
            "mode": "REQUIRED",
            "description": "Timestamp when the embedding was uploaded",
        },
    ]

    # Add updated_at timestamp to the DataFrame
    df["updated_at"] = datetime.utcnow()

    # Upload the DataFrame to BigQuery
    gcp_utils.bigquery.upload_dataframe_to_table(
        df=df,
        dataset_id=dataset_id,
        table_id=table_id,
        if_exists="replace",  # Replace the existing table
        schema_updates=schema,
    )

    print("Successfully uploaded user clusters to BigQuery")


def main():
    """Main execution function"""
    user_clusters_path = "/tmp/transformed/user_clusters/user_clusters.parquet"

    # todo: remove this hardcoded path
    # Load GCP credentials from environment or file
    credentials_path = "/home/dataproc/recommendation-engine/credentials_stage.json"
    credentials_json = ""

    if credentials_path and os.path.exists(credentials_path):
        with open(credentials_path, "r") as f:
            credentials_json = f.read()
    else:
        credentials_json = os.environ.get("GCP_CREDENTIALS_JSON", "")

    if not credentials_json:
        raise ValueError("GCP credentials not found in environment variables or file")

    # Define BigQuery table info
    dataset_id = "stage_test_tables"
    table_id = "test_user_cluster_embeddings"

    # Initialize GCP utils
    gcp_utils = GCPUtils(gcp_credentials=credentials_json)

    # Load the user clusters data
    df_clusters = load_user_clusters(user_clusters_path)

    # Upload to BigQuery
    upload_to_bigquery(df_clusters, gcp_utils, dataset_id, table_id)

    print("Process completed successfully")


if __name__ == "__main__":
    main()
