"""
Input:
- video_interaction_average.parquet (output from get_average_of_video_interactions.py)
- user_temporal_cluster_embeddings.parquet (output from get_temporal_interaction_embedding.py)
- user_video_clusters_distribution.parquet (output from get_user_video_cluster_distribution.py)

Output:
- merged_user_embeddings.parquet (combined user embeddings from all three sources)

This script merges the embeddings generated by different parts of the pipeline to create
a comprehensive embedding vector for each user.
"""

import os
import subprocess
import sys
import pathlib
import subprocess
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
import numpy as np
from pyspark.ml.feature import Normalizer
from pyspark.ml.linalg import Vectors, VectorUDT

# Initialize Spark Session
spark = SparkSession.builder.appName("Merge Part Embeddings").getOrCreate()

# Define constants
DATA_ROOT = "/home/dataproc/recommendation-engine/data_root"


# UDFs for embedding operations
def array_to_vector(array):
    return Vectors.dense(array)


array_to_vector_udf = F.udf(array_to_vector, VectorUDT())


def vector_to_array(vector):
    return vector.toArray().tolist()


vector_to_array_udf = F.udf(vector_to_array, ArrayType(FloatType()))


def concatenate_embeddings(emb1, emb2, emb3):
    """
    Concatenate three embedding arrays into one using NumPy.

    Args:
        emb1: First embedding array
        emb2: Second embedding array
        emb3: Third embedding array

    Returns:
        Concatenated array
    """
    if emb1 is None or emb2 is None or emb3 is None:
        return None

    try:
        # Convert to numpy arrays
        emb1_np = np.array(emb1)
        emb2_np = np.array(emb2)
        emb3_np = np.array(emb3)

        # Use numpy's concatenate function
        concatenated = np.concatenate([emb1_np, emb2_np, emb3_np])

        # Return as list for Spark compatibility
        return concatenated.tolist()
    except Exception as e:
        print(f"Error concatenating embeddings: {e}")
        return None


concatenate_embeddings_udf = F.udf(concatenate_embeddings, ArrayType(FloatType()))


def merge_part_embeddings(
    video_interaction_path, temporal_embeddings_path, cluster_distribution_path
):
    """
    Joins the three embeddings files and keeps only the embedding columns.

    Args:
        video_interaction_path: Path to video_interaction_average.parquet file
        temporal_embeddings_path: Path to user_temporal_cluster_embeddings.parquet file
        cluster_distribution_path: Path to user_video_clusters_distribution.parquet file

    Returns a dataframe with all embeddings joined by user_id.
    """
    print("STEP 1: Loading input data")

    # Load the three embedding dataframes
    df_video_interaction = spark.read.parquet(video_interaction_path)
    df_temporal_embeddings = spark.read.parquet(temporal_embeddings_path)
    df_cluster_distribution = spark.read.parquet(cluster_distribution_path)

    # Print schema and counts for debugging
    print("Video interaction average count:", df_video_interaction.count())
    df_video_interaction.printSchema()

    print("Temporal embeddings count:", df_temporal_embeddings.count())
    df_temporal_embeddings.printSchema()

    print("Cluster distribution count:", df_cluster_distribution.count())
    df_cluster_distribution.printSchema()

    print("\nSTEP 2: Selecting and renaming embedding columns")

    # Select only the user_id and embeddings from each dataframe with clear names
    df_video_emb = df_video_interaction.select(
        "user_id", F.col("embedding").alias("avg_interaction_embedding")
    )

    df_temporal_emb = df_temporal_embeddings.select(
        "user_id", F.col("temporal_embedding").alias("temporal_embedding")
    )

    df_cluster_emb = df_cluster_distribution.select(
        "user_id",
        F.col("cluster_distribution").alias("cluster_distribution_embedding"),
        "engagement_metadata_list",
    )

    print("\nSTEP 3: Joining dataframes on user_id")

    # Join all three dataframes on user_id
    # First join video embeddings with temporal embeddings
    df_joined = df_video_emb.join(df_temporal_emb, on="user_id", how="inner")

    # Then join with cluster distribution embeddings
    df_joined = df_joined.join(df_cluster_emb, on="user_id", how="inner")

    # Count users with all embeddings
    total_users = df_joined.count()
    print(f"Total users with all three embeddings: {total_users}")

    print("\nSTEP 4: Creating final output dataframe")

    # First select individual embeddings
    df_with_embeddings = df_joined.select(
        "user_id",
        "avg_interaction_embedding",
        "temporal_embedding",
        "cluster_distribution_embedding",
        "engagement_metadata_list",
    )

    print("\nSTEP 5: Concatenating embeddings")

    # Concatenate embeddings into a single user embedding
    df_with_concat = df_with_embeddings.withColumn(
        "concatenated_embedding",
        concatenate_embeddings_udf(
            F.col("avg_interaction_embedding"),
            F.col("temporal_embedding"),
            F.col("cluster_distribution_embedding"),
        ),
    )

    # Simply select the required columns for the final result
    df_result = df_with_concat.select(
        "user_id",
        F.col("concatenated_embedding").alias("user_embedding"),
        "avg_interaction_embedding",
        "temporal_embedding",
        "cluster_distribution_embedding",
        "engagement_metadata_list",
    )

    print("Final result count:", df_result.count())
    return df_result


def main():
    """Main execution function"""
    # Create necessary directories

    # Create local output directory if it doesn't exist
    local_output_dir = f"{DATA_ROOT}/emb_analysis"
    subprocess.call(["mkdir", "-p", local_output_dir])

    # Create hdfs directories, overwriting if they exist
    subprocess.call(["hdfs", "dfs", "-mkdir", "-p", "/tmp/emb_analysis"])

    # Define path variables
    video_interaction_path = "/tmp/emb_analysis/video_interaction_average.parquet"
    temporal_embeddings_path = (
        "/tmp/emb_analysis/user_temporal_cluster_embeddings.parquet"
    )
    cluster_distribution_path = (
        "/tmp/emb_analysis/user_video_clusters_distribution.parquet"
    )

    # Define input file mappings
    input_files = {
        "video_interaction_average.parquet": video_interaction_path,
        "user_temporal_cluster_embeddings.parquet": temporal_embeddings_path,
        "user_video_clusters_distribution.parquet": cluster_distribution_path,
    }

    # Copy input files to HDFS if they don't exist there already
    for local_file, hdfs_path in input_files.items():
        subprocess.call(
            [
                "hdfs",
                "dfs",
                "-put",
                "-f",
                f"{DATA_ROOT}/emb_analysis/{local_file}",
                hdfs_path,
            ]
        )

    # Merge the embeddings
    df_result = merge_part_embeddings(
        video_interaction_path, temporal_embeddings_path, cluster_distribution_path
    )

    # Write results to HDFS
    output_path = "/tmp/emb_analysis/merged_user_embeddings.parquet"
    df_result.write.mode("overwrite").parquet(output_path)

    # Create local output directory if not exists (double-check)
    subprocess.call(["mkdir", "-p", local_output_dir])

    # Copy results back to local filesystem
    subprocess.call(["hdfs", "dfs", "-get", "-f", output_path, local_output_dir])

    # Show a sample of data that was written
    print("\nSample of data written:")
    df_result.show(10)

    print(f"Successfully wrote merged user embeddings to {output_path}")
    print(f"And copied back to {local_output_dir}/merged_user_embeddings.parquet")


if __name__ == "__main__":
    main()
